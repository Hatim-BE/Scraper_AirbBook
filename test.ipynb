{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved to: cleaned_booking.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'booking.xlsx'\n",
    "\n",
    "# Define cleaning functions\n",
    "def clean_price(price):\n",
    "    # Remove all non-numeric characters (e.g., 'MAD', spaces)\n",
    "    return re.sub(r'[^\\d]', '', str(price))\n",
    "\n",
    "def remove_spaces(text):\n",
    "    # Remove extra spaces\n",
    "    return re.sub(r'\\s{2,}', ' ', str(text))\n",
    "\n",
    "def clean_ra_la_lo(ra_la_lo):\n",
    "    ra_la_lo = re.sub(r'[^\\d.,]', '', str(ra_la_lo))\n",
    "    # Remove extra spaces\n",
    "    return re.sub(r',', '.', str(ra_la_lo))\n",
    "\n",
    "def clean_column(df, col):\n",
    "    # If column contains 'price' in the name, clean it using clean_price\n",
    "    if 'price' in col.lower():\n",
    "        return df[col].apply(clean_price)\n",
    "\n",
    "    elif 'latiude' in col.lower() or 'longitude' in col.lower() or 'rating' in col.lower():\n",
    "        return df[col].apply(clean_ra_la_lo)\n",
    "    else:\n",
    "        return df[col]\n",
    "    \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "# Apply the cleaning function to all columns\n",
    "for col in df.columns:\n",
    "    df[col] = clean_column(df, col).apply(remove_spaces)\n",
    "\n",
    "# Save the cleaned data back to an Excel file\n",
    "cleaned_file_path = 'cleaned_booking.xlsx'\n",
    "df.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned file saved to: {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to the database was successful.\n"
     ]
    }
   ],
   "source": [
    "from utils.db import get_database_connection\n",
    "from collections import defaultdict\n",
    "\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from threading import Thread\n",
    "import time\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import mysql.connector\n",
    "\n",
    "global tables\n",
    "tables = ['booking_scraped_urls','airbnb_scraped_urls']\n",
    "\n",
    "def get_database_connection():\n",
    "    try:\n",
    "        conn = mysql.connector.connect(\n",
    "            host=os.getenv('DB_HOST'),\n",
    "            user=os.getenv('DB_USER'),\n",
    "            password=os.getenv('DB_PASSWORD'),\n",
    "            database=os.getenv('DB_NAME')\n",
    "        )\n",
    "        print(\"Connection to the database was successful.\")\n",
    "        return conn\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error connecting to database: {err}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_table_airbnb(conn):\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS airbnb_scraped_urls (\n",
    "                id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                serial TEXT UNIQUE NOT NULL\n",
    "                url VARCHAR(255) UNIQUE NOT NULL\n",
    "            )\n",
    "        ''')\n",
    "        conn.commit()\n",
    "        print(\"Tables `airbnb_scraped_urls` created successfully.\")\n",
    "        cursor.close()\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error creating tables ==> {err}\")\n",
    "\n",
    "def insert_url_airbnb(conn, url, ID):\n",
    "    try:\n",
    "        cursor = conn.cursor(buffer = True)\n",
    "        ID = ID.strip()\n",
    "        ID = str(ID)\n",
    "        cursor.execute(\"INSERT INTO airbnb_scraped_urls (url, serial) VALUES (%s, %s)\", (url, ID))\n",
    "        conn.commit()\n",
    "        print(f\"Inserted URL with ID ==> {ID}\")\n",
    "        cursor.close()\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error inserting airbnb ID: {ID}\")\n",
    "\n",
    "def check_url_exists_airbnb(conn, ID):\n",
    "        cursor = conn.cursor()\n",
    "        ID = ID.strip()\n",
    "        ID = str(ID)\n",
    "        cursor.execute(\"SELECT 1 FROM airbnb_scraped_urls WHERE serial = %s\", (ID,))\n",
    "        result = cursor.fetchone()\n",
    "        cursor.close()\n",
    "        return result is not None\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def save_to_xlsx_airbnb(listing_data, output_filename):\n",
    "    # Check if the file exists, and load existing data if it does\n",
    "    if os.path.exists(output_filename):\n",
    "        df = pd.read_excel(output_filename)\n",
    "    else:\n",
    "        # Define the DataFrame with the appropriate columns\n",
    "        df = pd.DataFrame(columns=['title', 'price', 'description', 'host', 'composition', 'rating', 'latitude', 'longitude', 'photo_links'])\n",
    "\n",
    "    # Ensure listing_data is a list of dictionaries or list of lists\n",
    "    if isinstance(listing_data, dict):  # If it's a single dictionary, wrap it in a list\n",
    "        listing_data = [listing_data]\n",
    "\n",
    "    # Convert the listing data into a DataFrame\n",
    "    df_new = pd.DataFrame(listing_data)\n",
    "    for col in df_new.columns:\n",
    "        df_new[col] = clean_column(df_new, col).apply(remove_spaces)\n",
    "        \n",
    "    \n",
    "    # Concatenate the new listings with the existing ones\n",
    "    df = pd.concat([df, df_new], ignore_index=True)\n",
    "    \n",
    "    # Save the DataFrame to the Excel file\n",
    "    df.to_excel(output_filename, index=False)\n",
    "    print(f\"Listings saved to {output_filename}\")\n",
    "\n",
    "\n",
    "def scrape_listing_details_airbnb(driver, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Adjust the wait time as needed\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Extract listing details (use the actual class names found from the inspection)\n",
    "    title = soup.find('h1', class_='hpipapi atm_7l_1kw7nm4 atm_c8_1x4eueo atm_cs_1kw7nm4 atm_g3_1kw7nm4 atm_gi_idpfg4 atm_l8_idpfg4 atm_kd_idpfg4_pfnrn2 i1pmzyw7 atm_9s_1nu9bjl dir dir-ltr').text.strip() if soup.find('h1', class_='hpipapi atm_7l_1kw7nm4 atm_c8_1x4eueo atm_cs_1kw7nm4 atm_g3_1kw7nm4 atm_gi_idpfg4 atm_l8_idpfg4 atm_kd_idpfg4_pfnrn2 i1pmzyw7 atm_9s_1nu9bjl dir dir-ltr') else 'mkynch'\n",
    "    price = soup.find('span', class_='_11jcbg2').text.strip() if soup.find('span', class_='_11jcbg2') else 'mkynch'\n",
    "    description = soup.find('div', class_='d1isfkwk atm_vv_1jtmq4 atm_w4_1hnarqo dir dir-ltr').text.strip() if soup.find('div', class_='d1isfkwk atm_vv_1jtmq4 atm_w4_1hnarqo dir dir-ltr') else 'mkynch'\n",
    "    host = soup.find('div', class_='t1pxe1a4').text.strip() if soup.find('div', class_='t1pxe1a4') else 'mkynch'\n",
    "    composition = soup.find('ol', class_='lgx66tx atm_gi_idpfg4 atm_l8_idpfg4 dir dir-ltr').text.strip() if soup.find('ol', class_='lgx66tx atm_gi_idpfg4 atm_l8_idpfg4 dir dir-ltr') else 'mkynch'\n",
    "    rating = soup.find('span', class_='_10nhpq7').text.strip() if soup.find('span', class_='_10nhpq7') else 'mkynch'\n",
    "    r = requests.get(url)\n",
    "    p_lat = re.compile(r'\"lat\":([-0-9.]+),')\n",
    "    p_lng = re.compile(r'\"lng\":([-0-9.]+),')\n",
    "    latitude = p_lat.findall(r.text)[0]\n",
    "    longitude = p_lng.findall(r.text)[0]\n",
    "\n",
    "\n",
    "\n",
    "    photo_links = [img['src'] for img in soup.find_all('img', class_='itu7ddv atm_e2_idpfg4 atm_vy_idpfg4 atm_mk_stnw88 atm_e2_1osqo2v__1lzdix4 atm_vy_1osqo2v__1lzdix4 i1cqnm0r atm_jp_pyzg9w atm_jr_nyqth1 i1de1kle atm_vh_yfq0k3 dir dir-ltr')]\n",
    "    photo_links = ','.join(photo_links)\n",
    "    \n",
    "    return {\n",
    "        'title': title,\n",
    "        'price': price,\n",
    "        'description': description,\n",
    "        'host': host,\n",
    "        'composition': composition,\n",
    "        'rating': rating,\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'photo_links': photo_links\n",
    "    }\n",
    "\n",
    "\n",
    "scraping_thread = None\n",
    "scraping_active_airbnb = True\n",
    "scraping_active_booking = True\n",
    "total_announcement_counts = 0\n",
    "airbnb_announcement_counts = 0\n",
    "booking_announcement_counts = 0\n",
    "\n",
    "cities = [\"rabat\"]\n",
    "conn = get_database_connection()\n",
    "listings_airbnb_by_city = {city: [] for city in cities}\n",
    "listings_booking_by_city = {city: [] for city in cities}\n",
    "\n",
    "def scrape_airbnb_urls_edge_airbnb(cities, conn):\n",
    "    global scraping_active_airbnb, total_announcement_counts, airbnb_announcement_counts\n",
    "    edge_options = Options()\n",
    "    # edge_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "    # Initialize the Edge WebDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Edge(service=service, options=edge_options)\n",
    "    global all_links\n",
    "    all_links = []  # List to store all extracted links\n",
    "\n",
    "    for city in cities:\n",
    "        if not scraping_active_airbnb:\n",
    "            break\n",
    "        print(f\"Scraping for {city}\")\n",
    "        base_url = f\"https://www.airbnb.com/s/{city}/homes\"\n",
    "        sum = 0\n",
    "        while base_url and scraping_active_airbnb:\n",
    "            if not scraping_active_airbnb:\n",
    "                break\n",
    "            driver.get(base_url)\n",
    "            \n",
    "            # Wait for JavaScript to load content\n",
    "            time.sleep(3)  # Adjust the wait time as needed\n",
    "\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Find and process the listing links\n",
    "            link_elements = soup.find_all('a', href=True, class_='l1ovpqvx atm_1he2i46_1k8pnbi_10saat9 atm_yxpdqi_1pv6nv4_10saat9 atm_1a0hdzc_w1h1e8_10saat9 atm_2bu6ew_929bqk_10saat9 atm_12oyo1u_73u7pn_10saat9 atm_fiaz40_1etamxe_10saat9 bn2bl2p atm_5j_223wjw atm_9s_1ulexfb atm_e2_1osqo2v atm_fq_idpfg4 atm_mk_stnw88 atm_tk_idpfg4 atm_vy_1osqo2v atm_26_1j28jx2 atm_3f_glywfm atm_kd_glywfm atm_3f_glywfm_jo46a5 atm_l8_idpfg4_jo46a5 atm_gi_idpfg4_jo46a5 atm_3f_glywfm_1icshfk atm_kd_glywfm_19774hq atm_uc_aaiy6o_1w3cfyq_oggzyc atm_70_1b8lkes_1w3cfyq_oggzyc atm_uc_glywfm_1w3cfyq_pynvjw atm_uc_aaiy6o_pfnrn2_ivgyl9 atm_70_1b8lkes_pfnrn2_ivgyl9 atm_uc_glywfm_pfnrn2_61fwbc dir dir-ltr')\n",
    "\n",
    "            if not link_elements:\n",
    "                print(\"No listings found. The class name may have changed or there are no listings available.\")\n",
    "            else:\n",
    "                print(f\"Found {len(link_elements)} listings on this page. Extracting URLs...\")\n",
    "                sum += len(link_elements)\n",
    "                \n",
    "                for link in link_elements:\n",
    "                    if not scraping_active_airbnb:\n",
    "                        break\n",
    "                    url = \"https://www.airbnb.com\" + link['href']\n",
    "                    part_after_slash = link['href'].split(\"/\")[-1]\n",
    "                    ID = part_after_slash.split(\"?\")[0] \n",
    "                    \n",
    "                    # Add the link and ID to the all_links list\n",
    "                    all_links.append((url, ID))\n",
    "\n",
    "            # Check and click the \"Next\" button\n",
    "            next_button = soup.find('a', attrs={\"aria-label\": \"hhh\"})\n",
    "            \n",
    "            if next_button:\n",
    "                next_url = next_button.get('href')\n",
    "                base_url = \"https://www.airbnb.com\" + next_url\n",
    "                print(f\"Navigating to next page:\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                base_url = None\n",
    "                print(\"No more pages to scrape.\")\n",
    "        print(f\"{sum} announcements scraped\")\n",
    "        print(f\"Finished scraping URLs for {city}\")\n",
    "\n",
    "    # Process all the links after scraping all cities\n",
    "    for url, ID in all_links:\n",
    "        if not scraping_active_airbnb:\n",
    "            break\n",
    "        print(f\"Processing URL: {url}\")\n",
    "        time.sleep(1)\n",
    "        if not check_url_exists_airbnb(conn, ID):\n",
    "            insert_url_airbnb(conn, url, ID)\n",
    "            total_announcement_counts += 1\n",
    "            airbnb_announcement_counts += 1\n",
    "            listing_data = scrape_listing_details_airbnb(driver, url)\n",
    "            output_filename = f\"test_airbnb.xlsx\"  # Assuming the same output file for all cities\n",
    "            save_to_xlsx_airbnb(listing_data, output_filename)\n",
    "        else:\n",
    "            print(f\"ID already exists: {ID}\")\n",
    "\n",
    "\n",
    "    driver.quit()\n",
    "    return all_links\n",
    "\n",
    "\n",
    "def get_all_links():\n",
    "    return all_links\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping for agadir\n",
      "Found 18 listings on this page. Extracting URLs...\n",
      "No more pages to scrape.\n",
      "18 announcements scraped\n",
      "Finished scraping URLs for agadir\n",
      "Processing URL: https://www.airbnb.com/rooms/1195165478582729878?adults=1&children=0&enable_m3_private_room=true&infants=0&pets=0&search_mode=regular_search&check_in=2024-09-14&check_out=2024-09-19&source_impression_id=p3_1724001661_P3xri7Nqu925Awev&previous_page_section_name=1000&federated_search_id=3b8ee6d0-b564-4c26-9b14-2d5f0a72e123\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Unread result found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscrape_airbnb_urls_edge_airbnb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 219\u001b[0m, in \u001b[0;36mscrape_airbnb_urls_edge_airbnb\u001b[1;34m(cities, conn)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    218\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_url_exists_airbnb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mID\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    220\u001b[0m     insert_url_airbnb(conn, url, ID)\n\u001b[0;32m    221\u001b[0m     total_announcement_counts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[4], line 72\u001b[0m, in \u001b[0;36mcheck_url_exists_airbnb\u001b[1;34m(conn, ID)\u001b[0m\n\u001b[0;32m     70\u001b[0m cursor\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT 1 FROM airbnb_scraped_urls WHERE serial = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, (ID,))\n\u001b[0;32m     71\u001b[0m result \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mfetchone()\n\u001b[1;32m---> 72\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mysql\\connector\\cursor.py:525\u001b[0m, in \u001b[0;36mMySQLCursor.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 525\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unread_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_result()\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mysql\\connector\\connection.py:1548\u001b[0m, in \u001b[0;36mMySQLConnection.handle_unread_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsume_results()\n\u001b[0;32m   1547\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munread_result:\n\u001b[1;32m-> 1548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InternalError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnread result found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mInternalError\u001b[0m: Unread result found"
     ]
    }
   ],
   "source": [
    "scrape_airbnb_urls_edge_airbnb(cities, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to the database was successful.\n"
     ]
    }
   ],
   "source": [
    "from utils.db import get_database_connection\n",
    "from collections import defaultdict\n",
    "\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from threading import Thread\n",
    "import time\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import mysql.connector\n",
    "\n",
    "global tables\n",
    "tables = ['booking_scraped_urls','airbnb_scraped_urls']\n",
    "\n",
    "def get_database_connection():\n",
    "    try:\n",
    "        conn = mysql.connector.connect(\n",
    "            host=os.getenv('DB_HOST'),\n",
    "            user=os.getenv('DB_USER'),\n",
    "            password=os.getenv('DB_PASSWORD'),\n",
    "            database=os.getenv('DB_NAME')\n",
    "        )\n",
    "        print(\"Connection to the database was successful.\")\n",
    "        return conn\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error connecting to database: {err}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_table_booking(conn):\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS booking_scraped_urls (\n",
    "                id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                serial TEXT UNIQUE NOT NULL\n",
    "                url VARCHAR(255) UNIQUE NOT NULL\n",
    "            )\n",
    "        ''')\n",
    "        conn.commit()\n",
    "        print(\"Tables `booking_scraped_urls` created successfully.\")\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error creating tables: {err}\")\n",
    "\n",
    "def insert_url_booking(conn, url, ID):\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"INSERT INTO booking_scraped_urls (url, serial) VALUES (%s, %s)\", (url, ID))\n",
    "        conn.commit()\n",
    "        print(f\"Inserted URL with ID:: {ID}\")\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error inserting URL: {err}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "def check_url_exists_booking(conn, url):\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT 1 FROM booking_scraped_urls WHERE url = %s\", (url,))\n",
    "        return cursor.fetchone() is not None\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error checking URL existence\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def save_to_xlsx_booking(listing_data, output_filename):\n",
    "    if os.path.exists(output_filename):\n",
    "        df = pd.read_excel(output_filename)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['url', 'title', 'price', 'description', 'host', 'composition', 'rating', 'latitude', 'longitude', 'photo_links'])\n",
    "\n",
    "    df_new = pd.DataFrame([listing_data])\n",
    "    df = pd.concat([df, df_new], ignore_index=True)\n",
    "    df.to_excel(output_filename, index=False)\n",
    "    print(f\"Listing saved to {output_filename}\")\n",
    "\n",
    "\n",
    "def scrape_listing_details_booking(driver, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Adjust the wait time as needed\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Extract listing details (use the actual class names found from the inspection)\n",
    "    title = soup.find('h2', class_='af32860db5 pp-header__title').text.strip() if soup.find('h2', class_='af32860db5 pp-header__title') else 'mkynch'\n",
    "    # First, find the td element\n",
    "    price = soup.find('span', class_='prco-valign-middle-helper')\n",
    "    if price:\n",
    "        price = price.text.strip()\n",
    "    else:\n",
    "        price = 0.0\n",
    "\n",
    "    description = soup.find('p', class_='e2585683de c8d1788c8c').text.strip() if soup.find('p', class_='e2585683de c8d1788c8c') else 'mkynch'\n",
    "    host = soup.find('div', class_='t1pxe1a4').text.strip() if soup.find('div', class_='t1pxe1a4') else 'mkynch'\n",
    "    composition = soup.find('div', class_='hprt-roomtype-bed').text.strip() if soup.find('div', class_='hprt-roomtype-bed') else 'mkynch'\n",
    "    rating = soup.find('div', class_='d0522b0cca fd44f541d8').find('div', class_='a447b19dfd').next_sibling.strip() if soup.find('div', class_='d0522b0cca fd44f541d8') else 0.0\n",
    "    latitude = \"mkynch\"\n",
    "    longitude = \"mkynch\"\n",
    "    map_element = soup.find('a', class_=\"loc_block_link_underline_fix\")\n",
    "    if map_element:\n",
    "        if \"data-atlas-latlng\" in map_element.attrs:\n",
    "            coordinates = map_element[\"data-atlas-latlng\"]\n",
    "            # Split the coordinates by comma\n",
    "            coords = coordinates.split(',')\n",
    "            latitude, longitude = coords\n",
    "    photo_links = []\n",
    "\n",
    "    try:\n",
    "        # Open the target URL\n",
    "        driver.get(url)\n",
    "        \n",
    "        try:\n",
    "            # Find and click the link to load more photos\n",
    "            photos_link = driver.find_element(By.CSS_SELECTOR, 'a.bh-photo-grid-item.bh-photo-grid-thumb.js-bh-photo-grid-item-see-all')\n",
    "            photos_link.click()\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding or clicking the photo link: {e}\")\n",
    "            return photo_links  # Return the initialized but empty list\n",
    "        \n",
    "        # Wait for the photos to load\n",
    "        time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            # Get the updated page source after the click\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            sources = [\"src\", \"srcset\", \"data-srcset\", \"data-src\"]\n",
    "            img_elements = soup.find_all('img', class_='bh-photo-modal-grid-image')\n",
    "            if img_elements:\n",
    "                for img in img_elements:\n",
    "                    for attr in sources:\n",
    "                        if attr in img.attrs:\n",
    "                            photo_links.append(img[attr])\n",
    "                            break  \n",
    "            \n",
    "                # Convert the list to a comma-separated string\n",
    "                photo_links = [','.join(photo_links)]\n",
    "            else:\n",
    "                print(\"No image elements found.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing images: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during the scraping process: {e}\")\n",
    "\n",
    "    print(photo_links)\n",
    "\n",
    "    return {\n",
    "        'url': url,\n",
    "        'title': title,\n",
    "        'price': price,\n",
    "        'description': description,\n",
    "        'host': host,\n",
    "        'composition': composition,\n",
    "        'rating': rating,\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'photo_links': photo_links\n",
    "    }\n",
    "\n",
    "\n",
    "scraping_thread = None\n",
    "scraping_active_airbnb = True\n",
    "scraping_active_booking = True\n",
    "global total_announcement_counts, airbnb_announcement_counts, booking_announcement_counts\n",
    "total_announcement_counts = 0\n",
    "airbnb_announcement_counts = 0\n",
    "booking_announcement_counts = 0\n",
    "\n",
    "cities = [\"agadir\", \"fes\", \"rabat\", \"casablanca\"]\n",
    "conn = get_database_connection()\n",
    "listings_airbnb_by_city = {city: [] for city in cities}\n",
    "listings_booking_by_city = {city: [] for city in cities}\n",
    "\n",
    "def scrape_booking(cities, conn):\n",
    "    global scraping_active_booking, total_url_counts, booking_url_counts, total_booking_url_counts\n",
    "    booking_url_counts = 0\n",
    "    total_booking_url_counts = 0\n",
    "    total_url_counts = 0\n",
    "\n",
    "    edge_options = Options()\n",
    "    # edge_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Edge(service=service, options=edge_options)\n",
    "\n",
    "    links = []\n",
    "\n",
    "    for city in cities:\n",
    "        if not scraping_active_booking:\n",
    "            break\n",
    "        print(f\"Scraping for {city}\")\n",
    "        base_url = f\"https://www.booking.com/searchresults.fr.html?ss={city}&checkin=2024-09-01&checkout=2024-09-02&group_adults=1&no_rooms=1&group_children=0\"\n",
    "        sum = 0\n",
    "        print(f\"while base_url and scraping_active_booking. {city}.....\")\n",
    "        if not scraping_active_booking:\n",
    "            break\n",
    "        driver.get(base_url)\n",
    "        time.sleep(5)\n",
    "        # Scroll down and click \"More results\" to load all listings\n",
    "        while True and scraping_active_booking:\n",
    "            print(\"while True and scraping_active_booking......\")\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            # Scroll to the bottom of the page\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)  # Adjust the wait time as needed\n",
    "\n",
    "            # Check if the height has increased\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            # If the height has increased, continue scrolling\n",
    "            if new_height > last_height:\n",
    "                last_height = new_height\n",
    "                continue  # Go back to scrolling\n",
    "\n",
    "            # If height hasn't increased, look for the \"More results\" button\n",
    "            try:\n",
    "                more_results_button = driver.find_element(By.CSS_SELECTOR, 'button.hhhhhdba1b3bddf.e99c25fd33.ea757ee64b.f1c8772a7d.ea220f5cdc.f870aa1234')\n",
    "                if more_results_button.is_displayed():  # Ensure the button is visible\n",
    "                    more_results_button.click()\n",
    "                    time.sleep(3)  # Adjust the wait time as needed\n",
    "                    last_height = driver.execute_script(\"return document.body.scrollHeight\")  # Update height after clicking\n",
    "                else:\n",
    "                    print(\"No 'More results' button found.\")\n",
    "                    break  # Exit the loop if button is not found or not clickable\n",
    "            except Exception as e:\n",
    "                print(f\"No more results to load or error occurred:\")\n",
    "                break  # Exit the loop if an exception occurs\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Try clicking the \"more results\" button to load more listings\n",
    "\n",
    "            \n",
    "        # Find and process the listing links\n",
    "        link_elements = soup.find_all('h3', class_='d3e8e3d21a')\n",
    "        for link_element in link_elements:\n",
    "                if not scraping_active_booking:\n",
    "                    break\n",
    "                links.append(link_element.find('a', href=True))\n",
    "        print(f\"Found {len(link_elements)} for {city}...\")\n",
    "\n",
    "\n",
    "    if not links:\n",
    "        print(\"No listings found. The class name may have changed or there are no listings available.\")\n",
    "    else:\n",
    "        total_booking_url_counts = len(links)\n",
    "        print(f\"Found {len(links)} listings on this page. Extracting URLs...\")\n",
    "        sum += len(links)\n",
    "        output_filename = f\"test_{city}_booking.xlsx\"\n",
    "\n",
    "        regex = r\"sr_pri_blocks=([^&]+)\"\n",
    "        for link in links:\n",
    "            if not scraping_active_booking:\n",
    "                break\n",
    "            url = link['href']\n",
    "            print(f\"Found URL: {url}\")\n",
    "            match = re.search(regex, link[\"href\"])\n",
    "            if match:\n",
    "                sr_pri_blocks_id = match.group(1)\n",
    "                print(f\"Found ID: {sr_pri_blocks_id}\")\n",
    "            else:\n",
    "                print(\"not found match\")\n",
    "\n",
    "\n",
    "            if not check_url_exists_booking(conn, sr_pri_blocks_id):\n",
    "                insert_url_booking(conn, url, sr_pri_blocks_id)\n",
    "                total_url_counts += 1\n",
    "                booking_url_counts += 1\n",
    "                listing_data = scrape_listing_details_booking(driver, url)\n",
    "                save_to_xlsx_booking(listing_data, output_filename)\n",
    "            else:\n",
    "                print(f\"URL already exists: {url}\")\n",
    "\n",
    "        '''for link in links:\n",
    "            save_to_xlsx_booking(listing_data, output_filename)  '''\n",
    "            \n",
    "    print(sum)\n",
    "    print(\"Finished scraping and inserting URLs.\")\n",
    "    driver.quit()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping for agadir\n",
      "while base_url and scraping_active_booking. agadir.....\n",
      "while True and scraping_active_booking......\n",
      "while True and scraping_active_booking......\n",
      "while True and scraping_active_booking......\n",
      "No more results to load or error occurred:\n",
      "Found 75 for agadir...\n",
      "Scraping for fes\n",
      "while base_url and scraping_active_booking. fes.....\n",
      "while True and scraping_active_booking......\n",
      "while True and scraping_active_booking......\n",
      "while True and scraping_active_booking......\n",
      "No more results to load or error occurred:\n",
      "Found 75 for fes...\n",
      "Scraping for rabat\n",
      "while base_url and scraping_active_booking. rabat.....\n",
      "while True and scraping_active_booking......\n",
      "while True and scraping_active_booking......\n",
      "while True and scraping_active_booking......\n",
      "No more results to load or error occurred:\n",
      "Found 75 for rabat...\n",
      "Scraping for casablanca\n",
      "while base_url and scraping_active_booking. casablanca.....\n",
      "while True and scraping_active_booking......\n",
      "while True and scraping_active_booking......\n",
      "while True and scraping_active_booking......\n",
      "No more results to load or error occurred:\n",
      "Found 75 for casablanca...\n",
      "Found 300 listings on this page. Extracting URLs...\n",
      "Found URL: https://www.booking.com/hotel/ma/anza-sahil-appartement.fr.html?aid=304142&label=gen173nr-1FCAQoggJCDXNlYXJjaF9hZ2FkaXJIDVgEaIwBiAEBmAENuAEXyAEM2AEB6AEB-AEDiAIBqAIDuAK4oYS2BsACAdICJDU0ZTg2ZTE0LWE2OTctNDE0ZS1hY2Q2LWFhMjBhOTlhOWQwZNgCBeACAQ&ucfs=1&arphpl=1&checkin=2024-09-01&checkout=2024-09-02&group_adults=1&req_adults=1&no_rooms=1&group_children=0&req_children=0&hpos=1&hapos=1&sr_order=popularity&srpvid=32eb945c58430045&srepoch=1723928772&all_sr_blocks=795091101_339594627_1_0_0&highlighted_blocks=795091101_339594627_1_0_0&matching_block_id=795091101_339594627_1_0_0&sr_pri_blocks=795091101_339594627_1_0_0__14580&from_sustainable_property_sr=1&from=searchresults\n",
      "Found ID: 795091101_339594627_1_0_0__14580\n",
      "Inserted URL with ID:: 795091101_339594627_1_0_0__14580\n",
      "['https://cf.bstatic.com/xdata/images/hotel/max1024x768/574348160.jpg?k=03c1b64b6107eb4bbab7bd3359f6147a8ce0e5594164871e85e8e28882a4d599&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804705.jpg?k=b40a9aac7595a9384fdd8bad3a0d989d4bf19b36b1eb012284b78f55c79cfd3d&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/574348148.jpg?k=aadc8aa8361ca53469d76392ee88974821b54cc2310806991d1977772ceb4ae8&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/574348150.jpg?k=45864969d0f3eecb831e9003917788d1bbc007e44c0a37125f2d4f1c98da7dca&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804710.jpg?k=ad8f46bf9c5d1f129868b9d1bd75e11fb7faed0e155241b26b65d355166dc5b4&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/574348159.jpg?k=08ec670ca29f945d06dec073fd19cb56c787d53c71f9512fc6bd125c9c2f74fc&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/571142208.jpg?k=bc7224437c5e873b7ae805d6bf990f48cdb491539f50a413597484c002cdaa99&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/570978500.jpg?k=ed6456fc476f511ddc2627fa1ab5d658879a334f4c43c4843627480687bc9aa4&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/574348162.jpg?k=73fa37c042210938d91bde77f3db43c38cb417823133200f9499624ea5a527ec&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/574348163.jpg?k=c956a341ccbe91c81215c8864a61283f181c3e32af63b64a865515bd2066acb8&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804717.jpg?k=a8df4e993823f8eb0499077d2c43b6412b7be88840916e3f2ee821d545a19bc0&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804756.jpg?k=a12671d8830876659332c771706f89fc1ef413463a1a632a692d50e3129eede6&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/574348141.jpg?k=a040962eeb061f915a3231785904a559d41ebbcc3f4d4efd5b10f1a4ef9f9d1d&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/570979551.jpg?k=404e8db36b9cc46475c2e036a09c9ae6c0632b9875fb99493a63d17b9cd8fde2&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/570978634.jpg?k=81570cc4abdfae2cdb2eddf16960895771a5497667da7808c676bc01c5715972&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804695.jpg?k=150f1e78016daad58f0b492b9f9a5a09b5131682df8e23ae4e761cd0297db113&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/574348157.jpg?k=07564cb821463a7ad70938626ba7d7ba910bb6861a77e3f7a67b8edb354c8dd9&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/574348144.jpg?k=b566c314fef3694e3591ede3a1890ce24dce4a79b6c0b3fb5422ebe128673b4c&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/574348151.jpg?k=1e3f5939fa39a4583ea07c375dd273ae07b3d23e2e89c8c5eb6eb5b1ccce6bb0&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567806601.jpg?k=9e5446f5232f4fef57e31b6f10302a73cf4c206e27c0580225d4fc0a99209073&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/571141458.jpg?k=6e4646a794105dfc3434eb530fbb85c1fe1cfac961eb6833d8a53a20bbfd1c3e&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/570986679.jpg?k=b40446d8095830d6144d4edbe9c3dfbe556e5bd8614c4804ae3a57adbbab0758&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/570986612.jpg?k=0609e0632f5da7553bef1c60ba1e115870ce166d9a4cea8daecfecaba48d4d8f&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/570986490.jpg?k=bc4f02cf8a04281e4987fc10eb9d35ce3f90663360bca798070a743f5a402568&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/570986431.jpg?k=4cd5025574aa4bf5508f8e0d43f1683b0de4de7cbad43a7dbdc124bc7c9b2a7c&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/570986388.jpg?k=3e3261fbc43b61230531b0d0d8f2a40a073a0510efd655c486a5ed742ba8e7e1&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/570986302.jpg?k=e1f96fc9d14c376b780863a682ca23b9ff63270900fb11f984b00a9f8c797b05&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/570986254.jpg?k=ba20beef4495af1b4bbe4e9afde5a3155d750847d689b4a623de866ac45e0628&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/570986238.jpg?k=dfbe60394082101e111af674dd9c0d247706bafff93969880c00fef94ecc4fdc&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/570980150.jpg?k=f00eaa3c7cf63a8c6c38803b1847903f64737380f0d04329bc74c15ba6461c47&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/570979792.jpg?k=5ab5741779a5ccd8b380c07b9a6c21ce35fa701e613165ba4156ce226cc10ab2&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/570978043.jpg?k=c7684fb97cd4bdbb2af0618a232be8f3b04ea48c4c79d2ab07ea9742bc79ffa1&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/568506809.jpg?k=fcece1ad73996355beee54906890134a3146d539ab68d8239797b90d306d354d&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804583.jpg?k=d40f90607eccd6cd635369c744bcc9c934e112385c2b3073700005a1ac766d8e&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804661.jpg?k=7ba63ea000117b50008b65fa668cc15f4e2ea5b055943a1c257d884b9f72d20d&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804708.jpg?k=d13731bd406281e85a8622064bf4bcec146e7f56c60feeb4c4353aa31956acc6&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804724.jpg?k=6b530392c034561ed54fec5937be98cd72fc93cd280ece4ff090eb63b71ccb08&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804722.jpg?k=107f00c189393db447a6803168a5c6f1564f3afcd8fd942f68bbc9dbe203340c&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804711.jpg?k=f08685ef32fa22189d99cd6196c1eee7b42d1017925b26c33010341f0f626cc9&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804716.jpg?k=7f00744f39e9a60f7d988695bce300a6afe6879a4cc20d504253ba521e958429&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804742.jpg?k=f1bf27be2ada20228e9b6031e31d8d939f0b7800bb09532013241da1dd08087d&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804739.jpg?k=812149be25fec99f355e715b92930ad57ef2e9975934fce6703cdcb47bd7904d&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804740.jpg?k=0ce929ef81f784523f2806506d9b1c1128012f8da8c55efa94516bc697ecd08e&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804741.jpg?k=7b85fcbac4bc7d865e76bb80cf602b8a3f0484c27bf756e587f07c498b32536e&o=&hp=1,https://cf.bstatic.com/xdata/images/hotel/max1024x768/567804737.jpg?k=9b32ac902080bee062955e454080a09d0ac87a51c107f58e4f425120193ea734&o=&hp=1']\n",
      "Listing saved to test_casablanca_booking.xlsx\n",
      "Found URL: https://www.booking.com/hotel/ma/les-palmiers-agadir.fr.html?aid=304142&label=gen173nr-1FCAQoggJCDXNlYXJjaF9hZ2FkaXJIDVgEaIwBiAEBmAENuAEXyAEM2AEB6AEB-AEDiAIBqAIDuAK4oYS2BsACAdICJDU0ZTg2ZTE0LWE2OTctNDE0ZS1hY2Q2LWFhMjBhOTlhOWQwZNgCBeACAQ&ucfs=1&arphpl=1&checkin=2024-09-01&checkout=2024-09-02&group_adults=1&req_adults=1&no_rooms=1&group_children=0&req_children=0&hpos=2&hapos=2&sr_order=popularity&srpvid=32eb945c58430045&srepoch=1723928772&all_sr_blocks=612373301_268091506_1_0_0&highlighted_blocks=612373301_268091506_1_0_0&matching_block_id=612373301_268091506_1_0_0&sr_pri_blocks=612373301_268091506_1_0_0__2200&from=searchresults\n",
      "Found ID: 612373301_268091506_1_0_0__2200\n",
      "Inserted URL with ID:: 612373301_268091506_1_0_0__2200\n",
      "Error finding or clicking the photo link: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"a.bh-photo-grid-item.bh-photo-grid-thumb.js-bh-photo-grid-item-see-all\"}\n",
      "  (Session info: chrome=127.0.6533.120); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00908923+23283]\n",
      "\t(No symbol) [0x008CE934]\n",
      "\t(No symbol) [0x00800733]\n",
      "\t(No symbol) [0x0084326F]\n",
      "\t(No symbol) [0x008434AB]\n",
      "\t(No symbol) [0x0087EE42]\n",
      "\t(No symbol) [0x00864464]\n",
      "\t(No symbol) [0x0087CB8D]\n",
      "\t(No symbol) [0x008641B6]\n",
      "\t(No symbol) [0x00838017]\n",
      "\t(No symbol) [0x0083890D]\n",
      "\tGetHandleVerifier [0x009FA5F3+1013699]\n",
      "\tGetHandleVerifier [0x00A03E4C+1052700]\n",
      "\tGetHandleVerifier [0x009FD4B4+1025668]\n",
      "\tGetHandleVerifier [0x0092EA2B+179195]\n",
      "\t(No symbol) [0x008D6833]\n",
      "\t(No symbol) [0x008D3198]\n",
      "\t(No symbol) [0x008D3337]\n",
      "\t(No symbol) [0x008CB4BE]\n",
      "\tBaseThreadInitThunk [0x771CFCC9+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x775D7C6E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x775D7C3E+238]\n",
      "\n",
      "Listing saved to test_casablanca_booking.xlsx\n",
      "Found URL: https://www.booking.com/hotel/ma/residence-tafat.fr.html?aid=304142&label=gen173nr-1FCAQoggJCDXNlYXJjaF9hZ2FkaXJIDVgEaIwBiAEBmAENuAEXyAEM2AEB6AEB-AEDiAIBqAIDuAK4oYS2BsACAdICJDU0ZTg2ZTE0LWE2OTctNDE0ZS1hY2Q2LWFhMjBhOTlhOWQwZNgCBeACAQ&ucfs=1&arphpl=1&checkin=2024-09-01&checkout=2024-09-02&group_adults=1&req_adults=1&no_rooms=1&group_children=0&req_children=0&hpos=3&hapos=3&sr_order=popularity&srpvid=32eb945c58430045&srepoch=1723928772&all_sr_blocks=203055901_295562342_0_0_0&highlighted_blocks=203055901_295562342_0_0_0&matching_block_id=203055901_295562342_0_0_0&sr_pri_blocks=203055901_295562342_0_0_0__2138&from=searchresults\n",
      "Found ID: 203055901_295562342_0_0_0__2138\n",
      "Inserted URL with ID:: 203055901_295562342_0_0_0__2138\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscrape_booking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 281\u001b[0m, in \u001b[0;36mscrape_booking\u001b[1;34m(cities, conn)\u001b[0m\n\u001b[0;32m    279\u001b[0m     total_url_counts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    280\u001b[0m     booking_url_counts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 281\u001b[0m     listing_data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_listing_details_booking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     save_to_xlsx_booking(listing_data, output_filename)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[1], line 118\u001b[0m, in \u001b[0;36mscrape_listing_details_booking\u001b[1;34m(driver, url)\u001b[0m\n\u001b[0;32m    114\u001b[0m photo_links \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Open the target URL\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;66;03m# Find and click the link to load more photos\u001b[39;00m\n\u001b[0;32m    122\u001b[0m         photos_link \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma.bh-photo-grid-item.bh-photo-grid-thumb.js-bh-photo-grid-item-see-all\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:363\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    362\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:352\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    350\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:302\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    300\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[0;32m    301\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:322\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    319\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 322\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\_request_methods.py:118\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    111\u001b[0m         method,\n\u001b[0;32m    112\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\_request_methods.py:217\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    213\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[0;32m    215\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    807\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\http\\client.py:1419\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1419\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1420\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1421\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "scrape_booking(cities, conn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved to: cleaned_booking.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'booking.xlsx'\n",
    "\n",
    "# Define cleaning functions\n",
    "def clean_price(price):\n",
    "    # Remove all non-numeric characters (e.g., 'MAD', spaces)\n",
    "    return re.sub(r'[^\\d]', '', str(price))\n",
    "\n",
    "def remove_spaces(text):\n",
    "    # Remove extra spaces\n",
    "    return re.sub(r'\\s{2,}', ' ', str(text))\n",
    "\n",
    "def clean_ra_la_lo(ra_la_lo):\n",
    "    ra_la_lo = re.sub(r'[^\\d.,]', '', str(ra_la_lo))\n",
    "    # Remove extra spaces\n",
    "    return re.sub(r',', '.', str(ra_la_lo))\n",
    "\n",
    "def clean_column(df, col):\n",
    "    # If column contains 'price' in the name, clean it using clean_price\n",
    "    if 'price' in col.lower():\n",
    "        return df[col].apply(clean_price)\n",
    "\n",
    "    elif 'latiude' in col.lower() or 'longitude' in col.lower() or 'rating' in col.lower():\n",
    "        return df[col].apply(clean_ra_la_lo)\n",
    "    else:\n",
    "        return df[col]\n",
    "    \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "# Apply the cleaning function to all columns\n",
    "for col in df.columns:\n",
    "    df[col] = clean_column(df, col).apply(remove_spaces)\n",
    "\n",
    "# Save the cleaned data back to an Excel file\n",
    "cleaned_file_path = 'cleaned_booking.xlsx'\n",
    "df.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned file saved to: {cleaned_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
